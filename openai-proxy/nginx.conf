events {
    worker_connections 1024;
}

http {
    upstream openai_backend {
        server api.openai.com:443;
    }

    # Cache configuration
    proxy_cache_path /tmp/nginx_cache levels=1:2 keys_zone=openai_cache:10m max_size=1g inactive=60m;
    
    # Rate limiting
    limit_req_zone $binary_remote_addr zone=openai_limit:10m rate=10r/m;

    server {
        listen 80;
        server_name localhost;

        # Health check endpoint
        location /health {
            access_log off;
            return 200 "healthy\n";
            add_header Content-Type text/plain;
        }

        # Proxy OpenAI API requests
        location /v1/ {
            limit_req zone=openai_limit burst=5 nodelay;
            
            # Cache GET requests for completions (optional)
            proxy_cache openai_cache;
            proxy_cache_valid 200 5m;
            proxy_cache_key "$request_method$request_uri$request_body";
            
            # Proxy settings
            proxy_pass https://openai_backend;
            proxy_ssl_server_name on;
            proxy_ssl_verify off;
            
            # Headers
            proxy_set_header Host api.openai.com;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
            
            # Timeout settings
            proxy_connect_timeout 30s;
            proxy_send_timeout 30s;
            proxy_read_timeout 30s;
            
            # Buffer settings
            proxy_buffering on;
            proxy_buffer_size 4k;
            proxy_buffers 8 4k;
        }

        # Metrics endpoint (basic)
        location /metrics {
            access_log off;
            return 200 "# OpenAI Proxy Metrics\nopenai_proxy_requests_total{status=\"ok\"} 1\n";
            add_header Content-Type text/plain;
        }

        # Default location
        location / {
            return 404 "OpenAI Proxy - Use /v1/ endpoints";
        }
    }
}